{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Assignment2_Derechowski_Artur.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/kartoniks/Studia/blob/master/Assignment2_Derechowski_Artur.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CGXgWugfJ0Vl",
        "colab_type": "text"
      },
      "source": [
        "# Assignment 2\n",
        "\n",
        "**Submission deadline: last lab session before or on Thursday, 26.03.2020**\n",
        "\n",
        "**Points: 6 + 1 bonus points**\n",
        "\n",
        "## Submission instructions\n",
        "The class is held remotely. To sumbmit your solutions please save the notebook to your Google Drive, then:\n",
        "1. Rename it it to: Assignment2_Surname_FirstName\n",
        "2. Rerun the whole notebook `Runtime -> Restar and run all`\n",
        "3. Make a pinned revision `File->Save and pin revision`\n",
        "4. Share the notebook with your instructor using his `cs.uni.wroc.pl` email\n",
        "\n",
        "We will use the commenting system and video conferences to check and discuss the solutions.\n",
        "\n",
        "As always, please submit corrections using GitHub's Pull Requests."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nfVDe-bMqVT_",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "%matplotlib inline"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiTEWD2oqW0Y",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RZCM_hdELE04",
        "colab_type": "text"
      },
      "source": [
        "The code below contains a mock-up of a two-layer neural network. Fill in the code and manually set weights to solve the XOR problem.\n",
        "\n",
        "Please note: the shapes are set to be compatible with PyTorch's conventions:\n",
        "* a batch containing $N$ $D$-dimensional examples has shape $N\\times D$ (each example is a row!)\n",
        "* a weight matrix in a linear layer with $I$ inputs and $O$ outputs has shape $O \\times I$\n",
        "* a bias vector is a 1D vector. Please note that [broadcasting rules](https://docs.scipy.org/doc/numpy/user/basics.broadcasting.html) allow us to think about it as a $1 \\times D` matrix."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lYEbCfbSpv5M",
        "colab_type": "code",
        "outputId": "547ea9ad-3c7d-489e-f0a0-be48b04df059",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 296
        }
      },
      "source": [
        "# Let's define a XOR dataset\n",
        "\n",
        "# X will be matrix of N 2-dimensional inputs\n",
        "X = np.array(\n",
        "    [[0, 0],\n",
        "     [0, 1],\n",
        "     [1, 0],\n",
        "     [1, 1],\n",
        "    ], dtype=np.float32)\n",
        "# Y is a matrix of N numners - answers\n",
        "Y = np.array(\n",
        "    [[0],\n",
        "     [1],\n",
        "     [1],\n",
        "     [0],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "plt.scatter(X[:,0], X[:,1], c=Y[:,0], )\n",
        "plt.xlabel('X[0]')\n",
        "plt.ylabel('X[1]')"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "Text(0, 0.5, 'X[1]')"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYIAAAEGCAYAAABo25JHAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nkKTGGQSS1Lj/B7qld7UMS/7bAAAAAElFTkSuQmCC\n",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Rb3azMn929_I",
        "colab_type": "text"
      },
      "source": [
        "# Problem 1 [2p]\n",
        "\n",
        "Fill in the details of a forward pass, then manually set the weights and biases in the network to solve the 2D XOR task defined above."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "lrrRuk6zLiF0",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def sigmoid(x):\n",
        "    return 1/(1 + np.exp(-x)) #DONE\n",
        "\n",
        "class SmallNet:\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
        "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
        "        self.b2 = np.zeros((1,), dtype=dtype)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        # DONE Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        self.W1 = np.random.rand(self.W1.shape[0], self.W1.shape[1])-0.5\n",
        "        self.b1 = np.random.rand(self.b1.shape[0],)-0.5\n",
        "        self.W2 = np.random.rand(1, self.W2.shape[1])-0.5\n",
        "        self.b2 = np.random.rand(1,)-0.5\n",
        "        pass\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        # TODO Problem 1: Fill in details of forward propagation\n",
        "\n",
        "        # Input to neurons in 1st layer\n",
        "        A1 = X @ self.W1.T + self.b1\n",
        "        # print(A1)\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O1 = sigmoid(A1)\n",
        "        # Inputs to neuron in the second layer\n",
        "        # print(O1)\n",
        "        A2 = O1 @ self.W2.T + self.b2\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O2 = sigmoid(A2)\n",
        "\n",
        "        if Y is not None:\n",
        "            # cross entropy loss\n",
        "            loss = - Y*np.log(O2) - (1-Y)*np.log(1-O2)\n",
        "            # normalize loss by batch size\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "            # TODO in Problem 2: fill in the gradient computation\n",
        "            # Please note, thate there is a correspondance between\n",
        "            # the forward and backward pass: with backward computations happening\n",
        "            # in reversed order. \n",
        "\n",
        "            # A2_grad is the gradient of loss with respect to A2\n",
        "            # Hint: there is a concise formula for the gradient \n",
        "            # of logistic sigmoid and cross-entropy loss \n",
        "            batch_size = X.shape[0]\n",
        "            O2_grad = - Y / (O2 + 1e-100) + (1-Y)/(1-O2+1e-100)\n",
        "            A2_grad = O2_grad * O2 * (1- O2) / batch_size\n",
        "            self.b2_grad = A2_grad.sum(0)\n",
        "            self.W2_grad = A2_grad.T @ O1\n",
        "            O1_grad = A2_grad * self.W2\n",
        "            A1_grad = O1_grad * O1 * (1 - O1)\n",
        "            self.b1_grad = A1_grad.sum(0)\n",
        "            self.W1_grad = A1_grad.T @ X\n",
        "            pass\n",
        "        return O2, loss"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jJswvBk0oiIY",
        "colab_type": "code",
        "outputId": "19faa45c-63af-4422-f669-a6f222b0dab2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        }
      },
      "source": [
        "# DONE Problem 1:\n",
        "# Set by hand the weight values to solve the XOR problem\n",
        "\n",
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "net.W1 = np.array([[10.,10.],[10.,10.]])\n",
        "net.b1 = np.array([-5.,-15.])\n",
        "net.W2 = np.array([[10.,-10.]])\n",
        "net.b2 = np.array([-5.])\n",
        "\n",
        "# Hint: since we use the logistic sigmoid activation, the weights may need to \n",
        "# be fairly large \n",
        "\n",
        "\n",
        "net.forward(X, Y, do_backward=True)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[0.00715279],\n",
              "        [0.99235586],\n",
              "        [0.99235586],\n",
              "        [0.00715279]]), 0.0074259970019481605)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wmxCi5Vl6_xB",
        "colab_type": "text"
      },
      "source": [
        "# Problem 2 [2p]\n",
        "\n",
        "1. Fill in the backward pass.\n",
        "2. Implement random initialization of network parameters.\n",
        "3. Using `float64` verify correctness of your backward pass."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "eSM5hgJ1mrhY",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def check_grad(net, param_name, X, Y, eps=1e-5):\n",
        "    \"\"\"A gradient checking routine\"\"\"\n",
        "    \n",
        "    param = getattr(net, param_name)\n",
        "    param_flat_accessor = param.reshape(-1)\n",
        "\n",
        "    grad = np.empty_like(param)\n",
        "    grad_flat_accessor = grad.reshape(-1)\n",
        "\n",
        "    net.forward(X, Y, do_backward=True)\n",
        "    orig_grad = getattr(net, param_name + '_grad')\n",
        "    assert (param.shape == orig_grad.shape)\n",
        "\n",
        "    for i in range(param_flat_accessor.shape[0]):\n",
        "        orig_val = param_flat_accessor[i]\n",
        "        param_flat_accessor[i] = orig_val + eps\n",
        "        _, loss_positive = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val - eps\n",
        "        _, loss_negative = net.forward(X, Y)\n",
        "        param_flat_accessor[i] = orig_val\n",
        "        grad_flat_accessor[i] = (\n",
        "            loss_positive - loss_negative) / (2 * eps)\n",
        "    assert np.allclose(grad, orig_grad)\n",
        "    return grad, orig_grad"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TTZu0jFEvgXF",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "net = SmallNet(2, 2, dtype=np.float64)\n",
        "\n",
        "for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "    check_grad(net, param_name, X, Y)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8mUOs3cVvjM2",
        "colab_type": "text"
      },
      "source": [
        "# Problem 3 [2p]\n",
        "\n",
        "Fill in the details of batch gradient descent below, then train a network to solve 2D XOR problem.\n",
        "\n",
        "Then test the reliability of solving the 3D XOR task."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "nn2AAoZo0vjU",
        "colab_type": "code",
        "outputId": "f565e356-4d13-4447-ad18-29bc9448f99d",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        }
      },
      "source": [
        "net = SmallNet(2, 10, dtype=np.float64)\n",
        "\n",
        "alpha = 1e-1\n",
        "\n",
        "for i in range(100000):\n",
        "    _, loss = net.forward(X, Y, do_backward=True)\n",
        "    if (i % 5000) == 0:\n",
        "        print(i, loss)\n",
        "    for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "        param = getattr(net, param_name)\n",
        "        grad = getattr(net, param_name+'_grad')\n",
        "        # Hint: use the construct `param[:]` to change the contents of the array!\n",
        "        # Doing instead `param = new_val` simply changes to what the variable\n",
        "        # param points to, without affecting the network!\n",
        "        param[:] = param - alpha*grad"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 0.7214894294565767\n",
            "5000 0.09035565668106316\n",
            "10000 0.01340624458614087\n",
            "15000 0.006756979042049487\n",
            "20000 0.004444429359132327\n",
            "25000 0.003288401769056047\n",
            "30000 0.002599914008490846\n",
            "35000 0.0021448879587223474\n",
            "40000 0.0018226102684420842\n",
            "45000 0.0015827981534937319\n",
            "50000 0.0013976186785228774\n",
            "55000 0.0012504479646805464\n",
            "60000 0.001130758589687894\n",
            "65000 0.001031566648806916\n",
            "70000 0.000948060341655601\n",
            "75000 0.0008768186058376111\n",
            "80000 0.0008153438130762401\n",
            "85000 0.0007617706898728552\n",
            "90000 0.0007146786159370482\n",
            "95000 0.0006729669156857783\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TwpEjpkU1JvK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 85
        },
        "outputId": "e209cc78-c026-40d9-f0da-f08019322759"
      },
      "source": [
        "net.forward(X, Y, do_backward=True)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(array([[5.18711593e-04],\n",
              "        [9.99448398e-01],\n",
              "        [9.99493493e-01],\n",
              "        [5.13202813e-04]]), 0.0005226425651728018)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "w3NiL0ku2Unr",
        "colab_type": "text"
      },
      "source": [
        "Generate below data for a 3D XOR task. Try a few values of hidden layer size. Plot the reliability of training, i.e. how many trainings succeed for a given hiden layer size.\n",
        "\n",
        "What is easier to train: a smaller, or large network?"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U0ZMyHqz8xrC",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 187
        },
        "outputId": "cf30f7f8-d858-4d32-eb85-d89fe059b947"
      },
      "source": [
        "X3 = np.array(\n",
        "    [[0, 0, 0],\n",
        "     [0, 1, 0],\n",
        "     [1, 0, 0],\n",
        "     [1, 1, 0],\n",
        "     [0, 0, 1],\n",
        "     [0, 1, 1],\n",
        "     [1, 0, 1],\n",
        "     [1, 1, 1],\n",
        "    ], dtype=np.float32)\n",
        "Y3 = np.array(\n",
        "    [[0],\n",
        "     [1],\n",
        "     [1],\n",
        "     [0],\n",
        "     [1],\n",
        "     [0],\n",
        "     [0],\n",
        "     [1],\n",
        "    ], dtype=np.float32)\n",
        "\n",
        "def train(hidden_no):\n",
        "    net = SmallNet(3, hidden_no, dtype=np.float64)\n",
        "    alpha = 1e-1\n",
        "\n",
        "    for i in range(100000):\n",
        "        _, loss = net.forward(X3, Y3, do_backward=True)\n",
        "        for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "            param = getattr(net, param_name)\n",
        "            grad = getattr(net, param_name+'_grad')\n",
        "            param[:] = param - alpha*grad\n",
        "    print(hidden_no, loss)\n",
        "    res, loss = net.forward(X3, Y3, do_backward=True)\n",
        "    # print(res)\n",
        "    good = 0\n",
        "    for i in range(len(Y3)):\n",
        "        if Y3[i] == np.round(res[i]):\n",
        "            good +=1\n",
        "    print(\"guess percent: \", good/len(Y3))\n",
        "\n",
        "\n",
        "for hidden_dim in [2, 3, 5, 10, 20]:\n",
        "    # TODO: run a few trainings and record the fraction of successful ones\n",
        "    train(hidden_dim)"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "2 0.6931271700009811\n",
            "guess percent:  0.5\n",
            "3 0.0016244232248809123\n",
            "guess percent:  1.0\n",
            "5 0.0007076094665467624\n",
            "guess percent:  1.0\n",
            "10 0.0005501996126055766\n",
            "guess percent:  1.0\n",
            "20 0.0006589225079548758\n",
            "guess percent:  1.0\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UuaLEoV-9DLG",
        "colab_type": "text"
      },
      "source": [
        "# Problem 4 [1bp]\n",
        "\n",
        "Replace the first nonlinearity with the [ReLU](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)) activation function. Verify ho"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bqTIWBD19ErK",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 357
        },
        "outputId": "a208d800-79a1-47c6-c489-4d4bee8295e6"
      },
      "source": [
        "def ReLU(x):\n",
        "    return np.maximum(0,x)\n",
        "\n",
        "def reluDerivative(x):\n",
        "    x[x<=0] = 0\n",
        "    x[x>0] = 1\n",
        "    return x\n",
        "\n",
        "class ReLUNet:\n",
        "    def __init__(self, in_features, num_hidden, dtype=np.float32):\n",
        "        self.W1 = np.zeros((num_hidden, in_features), dtype=dtype)\n",
        "        self.b1 = np.zeros((num_hidden,), dtype=dtype)\n",
        "        self.W2 = np.zeros((1, num_hidden), dtype=dtype)\n",
        "        self.b2 = np.zeros((1,), dtype=dtype)\n",
        "        self.init_params()\n",
        "\n",
        "    def init_params(self):\n",
        "        # DONE Problem 2:\n",
        "        # set all parameters to small random values, e.g. from N(0, 0.5)\n",
        "        self.W1 = np.random.rand(self.W1.shape[0], self.W1.shape[1])-0.5\n",
        "        self.b1 = np.random.rand(self.b1.shape[0],)-0.5\n",
        "        self.W2 = np.random.rand(1, self.W2.shape[1])-0.5\n",
        "        self.b2 = np.random.rand(1,)-0.5\n",
        "        pass\n",
        "\n",
        "    def forward(self, X, Y=None, do_backward=False):\n",
        "        # TODO Problem 1: Fill in details of forward propagation\n",
        "\n",
        "        # Input to neurons in 1st layer\n",
        "        A1 = X @ self.W1.T + self.b1\n",
        "        # print(A1)\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O1 = ReLU(A1)\n",
        "        # Inputs to neuron in the second layer\n",
        "        # print(O1)\n",
        "        A2 = O1 @ self.W2.T + self.b2\n",
        "        # Outputs after the sigmoid non-linearity\n",
        "        O2 = sigmoid(A2)\n",
        "\n",
        "        if Y is not None:\n",
        "            # cross entropy loss\n",
        "            loss = - Y*np.log(O2) - (1-Y)*np.log(1-O2)\n",
        "            # normalize loss by batch size\n",
        "            loss = loss.sum() / X.shape[0]\n",
        "        else:\n",
        "            loss = np.nan\n",
        "\n",
        "        if do_backward:\n",
        "            # TODO in Problem 2: fill in the gradient computation\n",
        "            # Please note, thate there is a correspondance between\n",
        "            # the forward and backward pass: with backward computations happening\n",
        "            # in reversed order. \n",
        "\n",
        "            # A2_grad is the gradient of loss with respect to A2\n",
        "            # Hint: there is a concise formula for the gradient \n",
        "            # of logistic sigmoid and cross-entropy loss \n",
        "            batch_size = X.shape[0]\n",
        "            O2_grad = - Y / (O2 + 1e-100) + (1-Y)/(1-O2+1e-100)\n",
        "            A2_grad = O2_grad * O2 * (1- O2) / batch_size\n",
        "            self.b2_grad = A2_grad.sum(0)\n",
        "            self.W2_grad = A2_grad.T @ O1\n",
        "            O1_grad = A2_grad * self.W2\n",
        "            A1_grad = O1_grad * reluDerivative(O1)\n",
        "            self.b1_grad = A1_grad.sum(0)\n",
        "            self.W1_grad = A1_grad.T @ X\n",
        "            pass\n",
        "        return O2, loss\n",
        "\n",
        "net = SmallNet(2, 10, dtype=np.float64)\n",
        "r_net = ReLUNet(2, 10, dtype=np.float64)\n",
        "alpha = 1e-1\n",
        "\n",
        "for i in range(100000):\n",
        "    _, loss = net.forward(X, Y, do_backward=True)\n",
        "    _, r_loss = r_net.forward(X, Y, do_backward=True)\n",
        "    if (i % 5000) == 0:\n",
        "        print(i, \"normal loss: \", loss, \"   ReLU loss: \", r_loss)\n",
        "    for param_name in ['W1', 'b1', 'W2', 'b2']:\n",
        "        param = getattr(net, param_name)\n",
        "        grad = getattr(net, param_name+'_grad')\n",
        "        r_param = getattr(r_net, param_name)\n",
        "        r_grad = getattr(r_net, param_name+'_grad')\n",
        "        param[:] = param - alpha*grad\n",
        "        r_param[:] = r_param - alpha*r_grad"
      ],
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0 normal loss:  0.6949112700440088    ReLU loss:  0.7098310247124273\n",
            "5000 normal loss:  0.25862409859235475    ReLU loss:  0.001823538747412883\n",
            "10000 normal loss:  0.018461379974080157    ReLU loss:  0.0007242890659516104\n",
            "15000 normal loss:  0.007868613749710533    ReLU loss:  0.0004346217603402281\n",
            "20000 normal loss:  0.0047974654047183    ReLU loss:  0.00030524882440037234\n",
            "25000 normal loss:  0.0033910181694076598    ReLU loss:  0.0002330355896274761\n",
            "30000 normal loss:  0.002597763224705745    ReLU loss:  0.00018733932810699274\n",
            "35000 normal loss:  0.0020931580619432616    ReLU loss:  0.0001559938171406502\n",
            "40000 normal loss:  0.0017459352494904176    ReLU loss:  0.00013324359289664816\n",
            "45000 normal loss:  0.0014934047546703404    ReLU loss:  0.0001160255427005763\n",
            "50000 normal loss:  0.0013020284629158875    ReLU loss:  0.00010256595819857717\n",
            "55000 normal loss:  0.001152314514697662    ReLU loss:  9.178068226495173e-05\n",
            "60000 normal loss:  0.001032194788390951    ReLU loss:  8.295026891782063e-05\n",
            "65000 normal loss:  0.0009338147733631339    ReLU loss:  7.559756406551575e-05\n",
            "70000 normal loss:  0.0008518512251089252    ReLU loss:  6.93882325735571e-05\n",
            "75000 normal loss:  0.0007825735927722909    ReLU loss:  6.407566248252392e-05\n",
            "80000 normal loss:  0.0007232928237211863    ReLU loss:  5.948481005825981e-05\n",
            "85000 normal loss:  0.0006720234726324344    ReLU loss:  5.5478246079186704e-05\n",
            "90000 normal loss:  0.0006272688841706614    ReLU loss:  5.1953932188366485e-05\n",
            "95000 normal loss:  0.0005878802735182879    ReLU loss:  4.883021146443055e-05\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}